# Robotics Final Project

## File Structure & Descriptions
```
---------------
+ = Python file
> = npy file
* = URDF file
$ = Folder

For more details, see comments in code
---------------

$ controllers
    $ grocery_shopper
        $ misc_old
            - test development
        $ old_good
            - keep map, waypoints, and checkpoints that have worked in path just in case
        $ old_urdfs
            - keep old URDF files just in case
        + config.py
            - Defines and initializes global variables, robot sensors, and robot state
        + controls.py
            - Contains the code for the manual controller and autonomous controller for driving the robot around the world
        + goal_object_detection.py
            - Uses openCV color detection to inform if a goal object is present in the frame.
        + grocery_shopper.py
            - This file is the main thread of execution and is the file that the Tiago Robot has set to the main controller. Based on the robot's state, the file calls upon other files to change the robot behavior. The main loop resides here.
        + helpers.py
            - Contains general functions that are used throughout the program.
        + manipulator.py
            - IK and manual manipulator combined. This gives user the control to adjust joints of the manipulator as well as providing functionality for the user to choose to use inverse kinematics to navigate the arm to positions.
        + mapping.py
            - Functionality for obtaining lidar readings and robot position and displaying them on a map. When mapping, this file also saves the position and lidar sensor information to a probability map, which is later used for path planning.
        + planner.py
            - Uses a navigation algorithm (RRT*) and the convolved map from the initial mapping phase to construct a path composed of waypoints. 
        + transformation.py
            - Contains functions for translating between map, world, and display coordinate systems.
        + trilateration.py
            - Uses cones and additional lidar to trilaterate position based on artificial noisy GPS measurement
        > checkpoints.npy
            - Contains a list of checkpoints that were generated in the mapping step. Each checkpoint either represents a user defined stopping point or a goal object detection.
        > map.npy
            - Probability map of lidar measurments for obstacles.
        > path.npy
            - List of waypoints inbetween checkpoints and current position that is generated by navigation algorithm.
        * tiago_urdf.urdf
            - Generated urdf file. Initially the given urdf file was used, but there were some parsing issues when using IKpy.
$ libraries
$ plugins
$ protos
$ worlds
    - grocery.wbt
        - modified wbt file for trilateration and better obstacle detection
```
### Flow of execution
1. Manual mapping with lidar sensor on robot
    * CV color recognition is used to add checkpoints on oject detection
    * User is also able to manually add waypoitns with keyboard press
2. User is able to go directly from this mode to then auonomously navigate facility using collected checkpoints
    * If robot gets stuck, user can arrest control from robot to drive it and then re assign control to autonomous controller
3. When a checkpoint is hit, control is given to the user to manipulate arm
    * In manipulation mode, user can change to manual drive mode or give control back to autonomous controller to locate to next waypoint
    * User can use keyboard press to locate to a basket position, use IK to go to a default position, or manually control

## Changes to .wbt File
* Additional Sensors
    * onboard_lidar_1
        * this is used for better mapping objects in the facility. Essentially, we just moved the existing lidar up to the top of the robot to avoid detecting the floor in the lidar obstacle measurements
    * slam_lidar_0
        * lidar for front 180 FOV of robot to detect cones in order for trilateration
    * slam_lidar_1
        * lidar for rear 180 FOV of robot to detect cones for trilateration algorithm

* Additional Objects
    * 18 Cone objects
        * Used as "landmark" objects or base stations in trilateration algorithm. Each has set position over all obstacles in the world so that robot has full sensing ability for these without having to take into consideration FOV blocking by obstacles.

## Resources Used

| Usage   | Links |
|--------------|-------------|
| Color detection          | https://github.com/lukicdarkoo/webots-example-visual-tracking/blob/master/controllers/visual_tracker/visual_tracker.py |
| Route planning          | https://en.wikipedia.org/wiki/A*_search_algorithm, https://fab.cba.mit.edu/classes/865.21/topics/path_planning/robotic.html|
| Trilateration          | https://www.alanzucconi.com/2017/03/13/positioning-and-trilateration/ |
| Various localization schemes          | https://github.com/nitinnat/Monte-Carlo-Localization/blob/master/Problem3_4.py, https://atsushisakai.github.io/PythonRobotics/modules/slam/ekf_slam/ekf_slam.html|
| Inverse Kinematics for manipulator arm          |https://gist.github.com/ItsMichal/4a8fcb330d04f2ccba582286344dd9a7, https://github.com/Phylliade/ikpy|


## Video Link
https://youtu.be/D_7RKOg03ls

